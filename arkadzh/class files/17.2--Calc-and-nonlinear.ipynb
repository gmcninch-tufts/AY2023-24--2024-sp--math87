{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Math087 - Mathematical Modeling\n",
    "===============================\n",
    "[Tufts University](http://www.tufts.edu) -- [Department of Math](http://math.tufts.edu)  \n",
    "[Arkadz Kirshtein](https://math.tufts.edu/people/facultyKirshtein.htm) <arkadz.kirshtein@tufts.edu>  \n",
    "*Fall 2023*\n",
    "\n",
    "Course material (Class 17): Calculus and nonlinear optimization background\n",
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we review the basic concepts required to understand and implement nonlinear optimization algorithms. Let us start with the review of some vector calculus concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directional derivative and the gradient\n",
    "------------------------------\n",
    "\n",
    "In single variable calculus the derivative of a function $f$ at a point $p$ is defined as a limit of the ration of difference between the value at $p$ and another point taken at distance $h$ from $p$ to the distance $h$: \n",
    "$$\\lim_{h\\rightarrow0}\\frac{f(p+h)-f(p)}{h}$$\n",
    "However if we consider a point in multiple dimensions, at a distance h instead of 2 options we get a circle, sphere or hypersphere depending on dimension. And consequently there are infinitely many directions in which we could step a distance h. This introduces a concept of directional derivative:\n",
    "\n",
    "Consider a unit (length=1) vector $\\mathbf{v}$. Then directional derivative of a function $f:\\ \\mathbb{R}^{n}\\mapsto \\mathbb{R}$ at a point $\\mathbf{p}$ in the direction of $\\mathbf{v}$ is a limit $$\\partial_{\\mathbf{v}}f=\\lim_{h\\rightarrow0}\\frac{f(\\mathbf{p}+h\\mathbf{v})-f(\\mathbf{p})}{h}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to compute the directional derivative. and as it turns out, we can utilize a vector of derivatives in the directions of each axis called partial derivatives. To take partial derivative one would treat all variables except the one in focus as constants and take \"regular\" derivative for the variable in question.\n",
    "\n",
    "For example to take the derivative with respect to $x$ of $f(x,y)=x^2y+xy^2$ one would treat $y$ as a constant and obtain \n",
    "$$\n",
    "\\partial_x(x^2y+xy^2)=\\partial_x(x^2)y+\\partial_x(x)y^2=(2x)y+(1)y^2=2xy+y^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In vector calculus, the gradient of a scalar-valued differentiable function f of several variables is the vector field (or vector-valued function) $\\nabla f$ whose value at a point $p$ is the vector whose components are the partial derivatives of $f$ at $p$. That is, for $f:\\ \\mathbb{R}^{n}\\mapsto \\mathbb{R}$, its gradient $\\nabla f:\\ \\mathbb{R}^{n}\\mapsto \\mathbb{R} ^{n}$ is defined at the point $p=(x_{1},\\dots ,x_{n})$ in n-dimensional space as the vector:\n",
    "$$\n",
    "\\nabla f=\n",
    "\\left[\\begin{array}{c}\n",
    "\\partial_{x_1}f\\\\\n",
    "\\partial_{x_2}f\\\\\n",
    "\\vdots\\\\\n",
    "\\partial_{x_n}f\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then to obtain directional derivative in the arbitrary direction one would take the inner product of the direction vector with the gradient field:\n",
    "$$\n",
    "\\partial_{\\mathbf{v}}f=\\mathbf{v}\\cdot\\nabla f.\n",
    "$$\n",
    "\n",
    "Additionally, because the inner product is related to the angle between vectors, the gradient vector incidentally provides the direction of the highest directional derivative (and it's opposite would be the lowest), thus being the base for the nonlinear minimization technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent\n",
    "---------\n",
    "\n",
    "Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. Conversely, stepping in the direction of the gradient will lead to a local maximum of that function; the procedure is then known as gradient ascent.\n",
    "\n",
    "*Gradient descent is generally attributed to Cauchy, who first suggested it in 1847. Hadamard independently proposed a similar method in 1907. Its convergence properties for non-linear optimization problems were first studied by Haskell Curry in 1944, with the method becoming increasingly well-studied and used in the following decades, also often called steepest descent.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since for $f$ the negative of the gradient provides the direction of the fastest decrease, the following iteration should theoretically lead to a local minimum:\n",
    "$$\n",
    "\\mathbf{x}^{n+1}=\\mathbf{x}^{n}-\\gamma\\nabla f(\\mathbf{x}^{n}).\n",
    "$$\n",
    "\n",
    "This works very similar to Newton's method. However since we are not looking for a zero of a function, we don't really know how far to step, thus introducing additional parameter $\\gamma$, choice of which is an independent topic for discussion.\n",
    "\n",
    "See [wiki](https://en.wikipedia.org/wiki/Gradient_descent) for gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonlinear least squares\n",
    "-------\n",
    "\n",
    "While linear version of least squares also utilizes gradient, it can be solved for the optimal value directly. In the case of nonlinear version one would implement gradient descent or similar algorithm to find the optimum. \n",
    "\n",
    "Specifically, consifer a vector valued function $\\mathbf{F}:\\ \\mathbb{R}^{n}\\mapsto \\mathbb{R} ^{m}$\n",
    "$$\n",
    "\\mathbf{F}(\\mathbf{x})=\\left[\\begin{array}{c}\n",
    "F_1(\\mathbf{x})\\\\\n",
    "F_2(\\mathbf{x})\\\\\n",
    "\\vdots\\\\\n",
    "F_m(\\mathbf{x})\n",
    "\\end{array}\\right],\\quad \\mathbf{x}=\\left[\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "\\vdots\\\\\n",
    "x_n\n",
    "\\end{array}\\right].\n",
    "$$\n",
    "\n",
    "Then to find the best version of $\\mathbf{x}$ such that $\\mathbf{F}(\\mathbf{x})\\approx \\mathbf{y}$, one would minimize $\\left\\|\\mathbf{F}(\\mathbf{x})- \\mathbf{y}\\right\\|^2$ leading to the following descent algorithm:\n",
    "$$\n",
    "\\mathbf{x}^{n+1}=\\mathbf{x}^{n}-\\gamma\\nabla(\\left\\|\\mathbf{F}(\\mathbf{x}^n)- \\mathbf{y}\\right\\|^2)=\\mathbf{x}^{n}-\\gamma\\nabla\\mathbf{F}(\\mathbf{x}^n)\\left(\\mathbf{F}(\\mathbf{x}^n)- \\mathbf{y}\\right),\n",
    "$$\n",
    "where $\\nabla\\mathbf{F}$ is an $n\\times m$ matrix of partial derivatives:\n",
    "$$\n",
    "\\nabla\\mathbf{F}(\\mathbf{x})=\\left[\\begin{array}{cccc}\n",
    "\\partial_{x_1}F_1(\\mathbf{x}),&\\partial_{x_1}F_2(\\mathbf{x}),&\\dots&\\partial_{x_1}F_m(\\mathbf{x})\\\\\n",
    "\\partial_{x_2}F_1(\\mathbf{x}),&\\partial_{x_2}F_2(\\mathbf{x}),&\\dots&\\partial_{x_2}F_m(\\mathbf{x})\\\\\n",
    "\\vdots&\\vdots&\\vdots&\\vdots\\\\\n",
    "\\partial_{x_n}F_1(\\mathbf{x}),&\\partial_{x_n}F_2(\\mathbf{x}),&\\dots&\\partial_{x_n}F_m(\\mathbf{x})\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
